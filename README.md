# ğŸ’³ Credit Card Fraud Detection â€” Public Project Overview (Businessâ€‘Oriented ML)

This is a **public showcase** of my finalâ€‘year BSc Computer Science thesis project: a **credit card fraud detection** system built with machine learning and evaluated with a **business / risk** mindset (metrics â†’ decisions â†’ operational impact).

> ğŸ” The full endâ€‘toâ€‘end implementation (weekâ€‘byâ€‘week reports, experiment logs, model artifacts) lives in a separate repository and can be shared on request (academic evaluation / recruitment).

---

## ğŸ§­ Executive Summary

Fraud detection is a **rareâ€‘event classification** problem where accuracy is misleading. The practical goal is to:

- **Stop fraud** (high **Recall/Fraud**, reduce fraud leakage)
- Keep **false alarms** operationally acceptable (reasonable **Precision/Fraud**, reduce customer friction + analyst workload)
- Treat the probability threshold as an **operational policy** (not â€œ0.5 by defaultâ€)

**Current champion model:** **XGBoost** (treeâ€‘based gradient boosting) with a **costâ€‘optimised threshold** selected on validation and applied once on the locked test set.

---

## ğŸ“Š Dataset

Kaggle â€œCredit Card Fraud Detectionâ€: **284,807** transactions over two days, **492 frauds (~0.17%)**.  
Features: anonymised PCA components **V1â€¦V28** plus **Time** and **Amount**.

**Why this matters:** with such imbalance, you must rely on **PRâ€‘AUC**, **costâ€‘based thresholding**, and careful validation/testing.

---

## ğŸ¦ Business translation (how we read the metrics)

- **TP (True Positives)** â†’ frauds stopped
- **FN (False Negatives)** â†’ frauds missed (direct loss / risk)
- **FP (False Positives)** â†’ false alarms (manual review cost + customer friction)

We therefore optimise and report results using an explicit cost policy:

- `cost_fp = 1`
- `cost_fn = 20`

---

## âœ… Results snapshot (locked test set)

**Operating threshold (valâ€‘selected, costâ€‘optimal):** **0.0884**  
At this operating point:

- **TP = 77**, **FP = 20**, **FN = 18**, **TN = 56,631**
- **Precision(Fraud) = 0.7938**, **Recall(Fraud) = 0.8105**
- **PRâ€‘AUC (AP) = 0.8171**

This is a strong, businessâ€‘friendly tradeâ€‘off: we catch most fraud while keeping false alarms low.

---

## ğŸ“ˆ Key Figures (recommended for the public overview)


### 1) Confusion Matrix (XGBoost at thr â‰ˆ 0.09)
![Confusion Matrix â€” XGBoost (thrâ‰ˆ0.09)](assets/week12_xgb_cm_test.png)

### 2) Cost vs Threshold (why thr=0.0884)
![Cost vs Threshold â€” XGBoost](assets/week12_xgb_cost_vs_threshold_test.png)

### 3) Precisionâ€“Recall Curve (PRâ€‘AUC)
![Precisionâ€“Recall Curve â€” XGBoost](assets/week12_xgb_pr_test.png)

---

## ğŸ” Explainability (Week 15 â€” SHAP)

To make the model **auditable and responsible**, SHAP (TreeExplainer) was applied to the champion XGBoost model.

**Global drivers (mean |SHAP|):** V4, V14, V8, V12, V15, V11, â€¦  
**Local case studies:**  
- **True Positive** (very high fraud probability)  
- **True Negative** (nearâ€‘zero fraud probability)  
- **Borderline** case near the operating threshold **0.0884**


![SHAP mean(|SHAP|) â€” Global Drivers](assets/shap_mean_abs_bar.png)

---

## ğŸ—‚ï¸ Project structure (overview)

- `src/` â€” scripts for data splitting, training, evaluation, explainability
- `models/` â€” saved model artifacts (joblib)
- `reports/` â€” weekly writeâ€‘ups + metrics/sweeps (full repo)
- `assets/` â€” **(this repo)** selected figures for the public overview

---

## âš¡ Reproducibility (high level)

1. Download `creditcard.csv` from Kaggle (not committed due to licensing).
2. Create stratified train/val/test splits (seeded).
3. Train and evaluate models (LogReg â†’ Decision Tree â†’ Random Forest â†’ XGBoost).
4. Select threshold on validation using cost policy; apply once on locked test.
5. Run SHAP for global + local explanations.

---

## ğŸ‘¤ Author

**Lazaros Voulistiotis** â€” finalâ€‘year Computer Science student, aspiring Machine Learning Engineer.

---
