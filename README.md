# ğŸ’³ Credit Card Fraud Detection â€” Public Project Overview (Businessâ€‘Oriented ML)

This is a **public showcase** of my finalâ€‘year BSc Computer Science thesis project: a **credit card fraud detection** system built with machine learning and evaluated with a **business / risk** mindset (metrics â†’ decisions â†’ operational impact).

> ğŸ” The full endâ€‘toâ€‘end implementation (weekâ€‘byâ€‘week reports, experiment logs, model artifacts) lives in a separate repository and can be shared on request (academic evaluation / recruitment).

---

## ğŸ§­ Executive Summary

Fraud detection is a **rareâ€‘event classification** problem where accuracy is misleading. The practical goal is to:

- **Stop fraud** (high **Recall/Fraud**, reduce fraud leakage)
- Keep **false alarms** operationally acceptable (high **Precision/Fraud**, reduce customer friction + analyst workload)
- Treat the probability threshold as an **operational policy** (not â€œ0.5 by defaultâ€)

**Current champion model:** **XGBoost** (treeâ€‘based gradient boosting).  
**Final operating policy (Month 4):** **precision constraint** (**precision â‰¥ 0.80**) selected on validation and applied once on the locked test set.

---

## ğŸ“Š Dataset

Kaggle â€œCredit Card Fraud Detectionâ€: **284,807** transactions over two days, **492 frauds (~0.17%)**.  
Features: anonymised PCA components **V1â€¦V28** plus **Time** and **Amount**.

**Why this matters:** with such imbalance, you must rely on **PRâ€‘AUC**, robust validation/testing, and a threshold policy aligned with operational constraints.

---

## ğŸ¦ Business translation (how we read the metrics)

- **TP (True Positives)** â†’ frauds stopped
- **FN (False Negatives)** â†’ frauds missed (direct loss / risk)
- **FP (False Positives)** â†’ false alarms (manual review cost + customer friction)

Instead of a default 0.5 threshold, the project treats the threshold as a **deployment decision**. During Month 4 we evaluated multiple policies (costâ€‘based, maxâ€‘F metrics, precision constraint) and **locked** the final policy below.

---

## âœ… Results snapshot (locked test set)

**Final operating policy:** `precision_constraint_p80` (precision â‰¥ 0.80)  
**Selected threshold (validation):** **0.1279**  
**Reported once on locked test set:**

- **TP = 77**, **FP = 16**, **FN = 18**, **TN = 56,635**
- **Precision(Fraud) = 0.8280**, **Recall(Fraud) = 0.8105**
- **F1 = 0.8191**, **F2 = 0.8140**
- **MCC = 0.8189** (strong singleâ€‘number metric under class imbalance)
- **PRâ€‘AUC (AP) = 0.8171**, **ROCâ€‘AUC = 0.9699**

**Business interpretation:** we catch **77/95** frauds while keeping false alarms extremely low (**16** false positives on ~56k legitimate transactions). This reduces fraud losses and keeps analyst workload/customer friction manageable.

---

## ğŸ“ˆ Key Figures

### 1) Confusion Matrix â€” XGBoost (final policy: precision â‰¥ 0.80)
![Confusion Matrix â€” XGBoost (precisionâ‰¥0.80)](assets/week16_confusion_matrix_test_p80.png)

### 2) Precisionâ€“Recall Curve (Validation) â€” threshold selected point
![PR Curve (VAL) â€” threshold selected](assets/week16_pr_curve_val_p80.png)

### 3) Explainability â€” SHAP (global drivers)
![SHAP mean(|SHAP|) â€” Global Drivers](assets/shap_mean_abs_bar.png)

### 4) Explainability â€” LIME (local case study)
![LIME local explanation (borderline case)](assets/week16_lime_borderline.png)

---

## ğŸ” Explainability (why it matters)

Fraud decisions affect customers (declines, stepâ€‘up verification) and must be explainable.

- **SHAP (TreeExplainer):** auditâ€‘grade global + local explanations for the treeâ€‘based champion model.
- **LIME (Tabular):** modelâ€‘agnostic, onâ€‘demand local explanations aligned with how analysts review flagged transactions.

Together, SHAP + LIME improve trust, investigation speed, and support responsible deployment.

---

## ğŸ—‚ï¸ Project structure (public overview)

- `assets/` â€” selected figures for the public overview (**this repo**)
- (Full repo) `src/`, `models/`, `reports/` â€” endâ€‘toâ€‘end implementation (private repo)

---

## âš¡ Reproducibility (high level)

1. Download `creditcard.csv` from Kaggle (not committed due to licensing).
2. Create stratified train/val/test splits (seeded).
3. Train and evaluate models (LogReg â†’ Decision Tree â†’ Random Forest â†’ XGBoost).
4. Select threshold on validation using a businessâ€‘aligned policy; apply once on locked test.
5. Run SHAP + LIME for global + local explanations.

---

## ğŸ‘¤ Author

**Lazaros Voulistiotis** â€” finalâ€‘year Computer Science student, aspiring Machine Learning Engineer.
